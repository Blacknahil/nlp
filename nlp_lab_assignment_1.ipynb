{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: Build a Combined English + Amharic Pipeline. Use paper based stop words, tokenizers and stemmers. Include the resources and papers you used in the colab.\n",
        "\n",
        "1.1 Detect language automatically\n",
        "\n",
        "1.2 Apply correct pipeline\n",
        "\n",
        "1.3 Output value"
      ],
      "metadata": {
        "id": "5N-rWWCqCwGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9Z07UW4Cw0a",
        "outputId": "184626d1-25a6-4aaa-9796-7d02933c30dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re, unicodedata\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iyATz6sDtL0",
        "outputId": "98e3119d-d7d9-4cfb-d095-dfd746b66fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## return type definition"
      ],
      "metadata": {
        "id": "1MQMixPIJiAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessedResult:\n",
        "  def __init__(self, lang, original_text, stemmed_text, lemmatized_text):\n",
        "    self.lang = lang\n",
        "    self.original_text = original_text\n",
        "    self.stemmed_text = stemmed_text\n",
        "    self.lemmatized_text = lemmatized_text"
      ],
      "metadata": {
        "id": "fsF7JWZjI_XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English Text processing and Normalization"
      ],
      "metadata": {
        "id": "MnptKqdBFfm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def english_preprocessing(text):\n",
        "  text = text.lower();\n",
        "\n",
        "  # remove punctation\n",
        "  no_punc_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  # tokenize based on words\n",
        "  tokens = word_tokenize(no_punc_text)\n",
        "\n",
        "  # stop word removal\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  no_stop_text = [ w for w in tokens if w not in stop_words]\n",
        "\n",
        "  # stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_text = [stemmer.stem(w) for w in no_stop_text]\n",
        "\n",
        "  # lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_text = [lemmatizer.lemmatize(w) for w in no_stop_text]\n",
        "\n",
        "  return PreProcessedResult(\"en\", text, stemmed_text,  lemmatized_text)\n",
        "\n",
        "eng_text = \"Hello! This is nahom, an incoming full-time software engineer at bloomberg!\"\n",
        "result = english_preprocessing(eng_text)\n"
      ],
      "metadata": {
        "id": "TaCWA4QxEOtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amharic Text Proccessing and Normalization"
      ],
      "metadata": {
        "id": "tTSpjOG7KUGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation\n",
        "def remove_punc(text):\n",
        "  # Ethiopic punctuation set\n",
        "  ethiopic_punct = \"፠፡።፣፤፥፦፧፨\"\n",
        "  # Remove all punctuation (ASCII + Ethiopic)\n",
        "  no_punct_text = re.sub(f\"[{ethiopic_punct}!?,;:]\", \"\", text)\n",
        "  return no_punct_text\n",
        "\n",
        "\n",
        "# stop word removal\n",
        "def remove_stop_words(text, stop_word_drive_path = \"/content/drive/MyDrive/datasets/amstopwords.txt\"):\n",
        "  stop_words = []\n",
        "  with open(stop_word_drive_path, \"r\") as f:\n",
        "    stop_words = f.read().splitlines()\n",
        "  no_stop_text = [w for w in text if w not in stop_words]\n",
        "  return no_stop_text\n",
        "\n"
      ],
      "metadata": {
        "id": "kunxRRf1I8Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# amharic stemmming\n",
        "stemming_suffixes = [\"ን\", \"ው\",\"ች\", \"ት\"]\n",
        "def simple_am_stem(word):\n",
        "    for s in stemming_suffixes:\n",
        "        if word.endswith(s):\n",
        "            return word[:-len(s)]\n",
        "    return word\n",
        "def amahric_stemming(text):\n",
        "  am_stemmed = [simple_am_stem(w) for w in text]\n",
        "  return am_stemmed"
      ],
      "metadata": {
        "id": "GS8grTQMNG-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# amahric lemmatization\n",
        "amharic_suffixes = [\n",
        "  \"ነው\", \"ሁሉ\", \"ች\", \"ዎች\", \"ን\", \"ው\", \"ም\", \"ኩ\", \"ህ\", \"ሽ\",\n",
        "  \"ኛ\", \"ኝ\", \"ት\", \"ያ\", \"ዋ\", \"ሁ\"\n",
        "]\n",
        "\n",
        "def lemmatize_word(word):\n",
        "  for suffix in sorted(amharic_suffixes, key=len, reverse=True):\n",
        "      if word.endswith(suffix):\n",
        "          return word[:-len(suffix)]\n",
        "  return word\n",
        "def amahric_lemmatizer(text):\n",
        "  lemmatized_text = [lemmatize_word(w) for w in text]\n",
        "  return lemmatized_text"
      ],
      "metadata": {
        "id": "nrwL_jhfOskQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def amaharic_preprocessing(text):\n",
        "  # amahric does not have lower and upper cases\n",
        "\n",
        "  # remove punctation\n",
        "  no_punc_text = remove_punc(text)\n",
        "\n",
        "  # tokenize based on words\n",
        "  tokens = no_punc_text.split()\n",
        "\n",
        "  # stop word removal\n",
        "  no_stop_text = remove_stop_words(tokens)\n",
        "\n",
        "  # stemming\n",
        "  stemmed_text = amahric_stemming(no_stop_text)\n",
        "\n",
        "  # lemmatization\n",
        "  lemmatized_text = amahric_lemmatizer(no_stop_text)\n",
        "\n",
        "  return PreProcessedResult(\"am\", text, stemmed_text, lemmatized_text)"
      ],
      "metadata": {
        "id": "J_G-FPWNOq3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## language detection"
      ],
      "metadata": {
        "id": "vXjvMWGtQOJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "from langdetect import detect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdAeqtbAcPFJ",
        "outputId": "44e7f0bf-1b22-4357-9b8e-566f13943918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def detect_language(text):\n",
        "  for c in text:\n",
        "    if 0x1200 <= ord(c) <= 0x137F:\n",
        "      return \"am\"\n",
        "    elif c.isascii() and c.isalpha():\n",
        "      return \"en\"\n",
        "\n",
        "  return \"unknown\"\n",
        "\n",
        "def detect_language_library(text):\n",
        "  return detect(text)"
      ],
      "metadata": {
        "id": "tmTeKDXPQP70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CombinedPipeline(text):\n",
        "  lang = detect_language(text)\n",
        "  if lang == \"am\":\n",
        "    ans = amaharic_preprocessing(text)\n",
        "  elif lang == \"en\":\n",
        "    ans = english_preprocessing(text)\n",
        "  else:\n",
        "    ans = PreProcessedResult(lang, text, [], [])\n",
        "  return ans"
      ],
      "metadata": {
        "id": "T5eGU2sVSr3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_text = '''ከአየር ጤና ወለቴ ያለው መንገድ ግንባታ ላይ እንደሆነ ይታወቃል።\n",
        "የአዲስ አበባ ከተማ መንገዶች ባለሥልጣን፤ \" በመገንባት ላይ የሚገኘውን የመንገድ ፕሮጀክት በፍጥነት ለማጠናቀቅ በከፍተኛ ትኩረት እየሰራው ነው \" ብሏል።\n",
        "'''\n",
        "result = CombinedPipeline(eng_text)\n",
        "\n",
        "print(result.lang)\n",
        "print(result.original_text)\n",
        "print(\"stemmed text: \", result.stemmed_text)\n",
        "print(\"lemmatized_text: \", result.lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBZAkQSFTQ6b",
        "outputId": "ac26b616-7ef0-42b4-d301-29b281266a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am\n",
            "ከአየር ጤና ወለቴ ያለው መንገድ ግንባታ ላይ እንደሆነ ይታወቃል።\n",
            "የአዲስ አበባ ከተማ መንገዶች ባለሥልጣን፤ \" በመገንባት ላይ የሚገኘውን የመንገድ ፕሮጀክት በፍጥነት ለማጠናቀቅ በከፍተኛ ትኩረት እየሰራው ነው \" ብሏል።\n",
            "\n",
            "stemmed text:  ['ከአየር', 'ጤና', 'ወለቴ', 'መንገድ', 'ግንባታ', 'እንደሆነ', 'ይታወቃል', 'የአዲስ', 'አበባ', 'ከተማ', 'መንገዶ', 'ባለሥልጣ', '\"', 'በመገንባ', 'የሚገኘው', 'የመንገድ', 'ፕሮጀክ', 'በፍጥነ', 'ለማጠናቀቅ', 'በከፍተኛ', 'ትኩረ', 'እየሰራ', '\"', 'ብሏል']\n",
            "lemmatized_text:  ['ከአየር', 'ጤና', 'ወለቴ', 'መንገድ', 'ግንባታ', 'እንደሆነ', 'ይታወቃል', 'የአዲስ', 'አበባ', 'ከተማ', 'መንገዶ', 'ባለሥልጣ', '\"', 'በመገንባ', 'የሚገኘው', 'የመንገድ', 'ፕሮጀክ', 'በፍጥነ', 'ለማጠናቀቅ', 'በከፍተ', 'ትኩረ', 'እየሰራ', '\"', 'ብሏል']\n"
          ]
        }
      ]
    }
  ]
}